{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arielhsieh8/cs-uy-4613-project/blob/milestone-4/Model_FineTune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCO9jo5gyX2c"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GHSa0Qb1xTvJ"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch \n",
        "from torch.utils.data import Dataset \n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments \n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the data \n",
        "\n",
        "model_name = \"distilbert-base-uncased\" #use distilbert based model to train \n",
        "\n",
        "train_data = pd.read_csv('train.csv') # read in data \n",
        "\n",
        "train_data.drop([\"id\"], inplace=True, axis=1) #drop id column because it is not a feature \n",
        "train_data.dropna() # drop rows that do not have any data \n",
        "\n",
        "train_texts = train_data['comment_text'].tolist() # create X dataset and convert to list \n",
        "train_labels = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values.tolist() # create y dataset and convert to list \n",
        "\n",
        "test_texts, test_labels = train_texts[100000:101000],train_labels[100000:101000] # test data is 1000 of unused data in train.csv \n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts[:100000],train_labels[:100000],test_size=0.20,random_state=42) # split up the data into 80% train, 20% validation"
      ],
      "metadata": {
        "id": "TIT87fYYaJB1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class textDataset to store a text and its associated labels \n",
        "\n",
        "class textDataset(Dataset):\n",
        "\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels).float()\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[index])\n",
        "        return item\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "YYrk3EKgh34S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sl36PcY2rxGX"
      },
      "outputs": [],
      "source": [
        "# define tokenizer from pretrained model \n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name,num_labels=6,problem_type=\"multi_label_classification\")\n",
        "\n",
        "#create the encodings of the training and validation dataset using the tokenizer \n",
        "train_encodings = tokenizer(train_texts,truncation=True,padding=True)\n",
        "val_encodings = tokenizer(val_texts,truncation=True,padding=True)\n",
        "\n",
        "#create the dataset of the training and validation sets with the class textDataset\n",
        "train_dataset = textDataset(train_encodings,train_labels)\n",
        "val_dataset = textDataset(val_encodings,val_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uyVppYpxJ7r"
      },
      "outputs": [],
      "source": [
        "#train the model with function, using 2 epochs and batch sizes of 16 \n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "#define the model from pretrained imported model \n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=6,problem_type=\"multi_label_classification\")\n",
        "\n",
        "#define the trainer and the dataset it is to be trained on \n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGigZhWtV0ld"
      },
      "outputs": [],
      "source": [
        "# train the model \n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lowGDIRRV2Kk"
      },
      "outputs": [],
      "source": [
        "# save the trained model in a directory \n",
        "save_directory = \"saved\"\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model on the test dataset \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "# import tokenizer \n",
        "tokenizer = AutoTokenizer.from_pretrained('Ariel8/toxic-tweets-classification')\n",
        "# import model \n",
        "model = AutoModelForSequenceClassification.from_pretrained('Ariel8/toxic-tweets-classification')\n",
        "\n",
        "p = []\n",
        "threshold = 0.5 # set the threshold of determining whether something is a 0 or 1 \n",
        "# loop through all the texts in the text dataset \n",
        "for i in range(len(test_texts)):\n",
        "  #tokenize each text with the imported tokenizer \n",
        "  batch = tokenizer(test_texts[i], truncation=True, padding='max_length', return_tensors=\"pt\") \n",
        "  with torch.no_grad():\n",
        "    outputs = model(**batch) # run the text through the model to get its predicted output \n",
        "    predictions = torch.sigmoid(outputs.logits) # get the probability of the output by running through sigmoid function \n",
        "    p.append((predictions >= threshold).int()) # if probability > 0.5 --> 1, else --> 0\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ef1kyRbOmRs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy \n",
        "\n",
        "count = 0 \n",
        "for i in range(len(p)): \n",
        "  x = np.concatenate(p[i].tolist(),axis=None) #change format of array to list and flatten \n",
        "  if list(x) == test_labels[i]: # if the list of predicted labels matches the given true labels, then increment the count \n",
        "    count += 1 \n",
        "print('Accuracy: ', count/len(p)) # get the accuracy "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URqpg9Yq9vLf",
        "outputId": "0796c449-2a0f-4c95-bea6-15772b8574e6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.936\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SI5wXUWiK-4VnrwWn6Pq2r2e3pzK15mn",
      "authorship_tag": "ABX9TyPd3l85OadP+BFI8sQYmQ1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}